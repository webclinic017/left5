{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "weird-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import copy\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn import tree, svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d18382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=8, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "saved-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91227c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcfc35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4e356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40 #100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3ed6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b4e4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc796524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FutureTrendModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FutureTrendModule, self).__init__()\n",
    "        \n",
    "        # input batch_size 300\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(900, 128),\n",
    "        )\n",
    "        \n",
    "        # input batch_size 1 128\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(True),\n",
    "#             nn.AvgPool1d(3, 2),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "        )\n",
    "        \n",
    "        # input batch_size 64 64\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "#             nn.AvgPool1d(3, 2),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "        )\n",
    "        \n",
    "        # input batch_size 32 32\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 16, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(True),\n",
    "#             nn.AvgPool1d(3, 2),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # input batch_size 256\n",
    "        self.output_fc1 = nn.Linear(256, 128)\n",
    "        # input batch_size 128\n",
    "        self.output_fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         return self.model(x)\n",
    "        fc1 = self.fc1(x).view(-1, 1, 128)\n",
    "        conv1 = self.conv1(fc1)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        dropout = self.dropout(conv3.view(-1, 256))\n",
    "        out_fc1 = self.output_fc1(dropout)\n",
    "        outs = self.output_fc2(out_fc1)\n",
    "        return outs, fc1, conv1, conv2, conv3, dropout, out_fc1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8914d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FutureTrendModule().to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "784401a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss().to(0)  # mean square error\n",
    "loss_fn = nn.SmoothL1Loss().to(0)\n",
    "# loss_fn = nn.CrossEntropyLoss().to(0)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.000001)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.000001, momentum=0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10f6ff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_PATH = 'D:/option/datas'\n",
    "# SAVE_PATH = 'D:/option/l8'\n",
    "SAVE_PATH = 'D:/option/l8_5mean_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77bcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/40], SHFE.rb.2022-12-02.41243.h5, Loss: 2.15137022\n",
      "Epoch: [1/40], SHFE.rb.2022-12-05.41281.h5, Loss: 2.27118863\n",
      "Epoch: [1/40], SHFE.rb.2022-12-06.40848.h5, Loss: 2.01426230\n",
      "Epoch: [1/40], SHFE.rb.2022-12-07.41234.h5, Loss: 2.27519231\n",
      "Epoch: [1/40], SHFE.rb.2022-12-08.41207.h5, Loss: 2.10357657\n",
      "Epoch: [1/40], SHFE.rb.2022-12-09.41358.h5, Loss: 2.42550006\n",
      "Epoch: [1/40], SHFE.rb.2022-12-12.41249.h5, Loss: 2.23307048\n",
      "Epoch: [1/40], SHFE.rb.2022-12-13.41244.h5, Loss: 2.38360830\n",
      "Epoch: [1/40], SHFE.rb.2022-12-14.41187.h5, Loss: 2.19051598\n",
      "Epoch: [1/40], SHFE.rb.2022-12-15.41351.h5, Loss: 2.63979245\n",
      "Epoch: [1/40], SHFE.rb.2022-12-16.41382.h5, Loss: 2.72954196\n",
      "Epoch: [1/40], SHFE.rb.2022-12-19.41305.h5, Loss: 2.59261314\n",
      "Epoch: [1/40], SHFE.rb.2022-12-20.41323.h5, Loss: 2.47904116\n",
      "Epoch: [1/40], SHFE.rb.2022-12-21.41294.h5, Loss: 2.46065731\n",
      "Epoch: [1/40], SHFE.rb.2022-12-22.41297.h5, Loss: 2.54562134\n",
      "Epoch: [1/40], SHFE.rb.2022-12-23.41363.h5, Loss: 2.49387843\n",
      "Epoch: [1/40], SHFE.rb.2022-12-26.41245.h5, Loss: 2.26607084\n",
      "Epoch: [1/40], SHFE.rb.2022-12-27.41203.h5, Loss: 2.28915051\n",
      "Epoch: [1/40], SHFE.rb.2022-12-28.41268.h5, Loss: 2.16450364\n",
      "Epoch: [1/40], SHFE.rb.2022-12-29.41263.h5, Loss: 2.12641653\n",
      "Epoch: [1/40], SHFE.rb.2022-12-30.26898.h5, Loss: 2.24792728\n",
      "Epoch: [1/40], SHFE.rb.2023-01-03.41159.h5, Loss: 1.88089714\n",
      "Epoch: [1/40], SHFE.rb.2023-01-04.41291.h5, Loss: 2.34597165\n",
      "Epoch: [1/40], SHFE.rb.2023-01-05.41261.h5, Loss: 2.23912739\n",
      "Epoch: [1/40], SHFE.rb.2023-01-06.41378.h5, Loss: 2.59702435\n",
      "Epoch: [1/40], SHFE.rb.2023-01-09.41028.h5, Loss: 1.98282600\n",
      "Epoch: [1/40], SHFE.rb.2023-01-10.41074.h5, Loss: 2.01040762\n",
      "Epoch: [1/40], SHFE.rb.2023-01-11.41239.h5, Loss: 1.87674881\n",
      "Epoch: [1/40], SHFE.rb.2023-01-12.41161.h5, Loss: 2.00645229\n",
      "Epoch: [1/40], SHFE.rb.2023-01-13.41112.h5, Loss: 1.63581426\n",
      "Epoch: [1/40], SHFE.rb.2023-01-16.41176.h5, Loss: 1.97727271\n",
      "Epoch: [1/40], SHFE.rb.2023-01-17.41091.h5, Loss: 1.82630738\n",
      "Epoch: [1/40], SHFE.rb.2023-01-18.40868.h5, Loss: 1.85432357\n",
      "Epoch: [1/40], SHFE.rb.2023-01-19.40989.h5, Loss: 1.94526273\n",
      "Epoch: [1/40], SHFE.rb.2023-01-20.26721.h5, Loss: 1.96134171\n",
      "Epoch: [1/40], SHFE.rb.2023-01-30.41190.h5, Loss: 2.17983166\n",
      "Epoch: [1/40], SHFE.rb.2023-01-31.41277.h5, Loss: 2.26158631\n",
      "Epoch: [1/40], SHFE.rb.2023-02-01.41291.h5, Loss: 2.37812822\n",
      "Epoch: [1/40], SHFE.rb.2023-02-02.41255.h5, Loss: 2.31548751\n",
      "Epoch: [1/40], SHFE.rb.2023-02-03.41366.h5, Loss: 2.41425523\n",
      "Epoch: [1/40], SHFE.rb.2023-02-06.41310.h5, Loss: 2.41378966\n",
      "Epoch: [1/40], SHFE.rb.2023-02-07.41324.h5, Loss: 2.45955367\n",
      "Epoch: [1/40], SHFE.rb.2023-02-08.41316.h5, Loss: 2.19259531\n",
      "Epoch: [1/40], SHFE.rb.2023-02-09.41266.h5, Loss: 2.01441580\n",
      "Epoch: [1/40], SHFE.rb.2023-02-10.41273.h5, Loss: 2.41634943\n",
      "Epoch: [1/40], SHFE.rb.2023-02-13.41342.h5, Loss: 2.20235362\n",
      "Epoch: [1/40], SHFE.rb.2023-02-14.41311.h5, Loss: 2.10627450\n",
      "Epoch: [1/40], SHFE.rb.2023-02-15.41189.h5, Loss: 1.96529676\n",
      "Epoch: [1/40], SHFE.rb.2023-02-16.41329.h5, Loss: 2.16486688\n",
      "Epoch: [1/40], SHFE.rb.2023-02-17.41290.h5, Loss: 2.02600716\n",
      "Epoch: [1/40], SHFE.rb.2023-02-20.41254.h5, Loss: 1.85645468\n",
      "Epoch: [1/40], SHFE.rb.2023-02-21.41275.h5, Loss: 2.17854130\n",
      "Epoch: [1/40], SHFE.rb.2023-02-22.41338.h5, Loss: 2.00576165\n",
      "Epoch: [1/40], SHFE.rb.2023-02-23.41315.h5, Loss: 2.29095711\n",
      "Epoch: [1/40], SHFE.rb.2023-02-24.41341.h5, Loss: 2.51372060\n",
      "Epoch: [1/40], SHFE.rb.2023-02-27.41294.h5, Loss: 2.06782338\n",
      "Epoch: [1/40], SHFE.rb.2023-02-28.41244.h5, Loss: 2.03839861\n",
      "Epoch: [1/40], SHFE.rb.2023-03-01.41250.h5, Loss: 2.01192081\n",
      "Epoch: [1/40], SHFE.rb.2023-03-02.41145.h5, Loss: 1.70795602\n",
      "Epoch: [1/40], SHFE.rb.2023-03-03.41219.h5, Loss: 1.87858952\n",
      "Epoch: [1/40], SHFE.rb.2023-03-06.41227.h5, Loss: 1.96163670\n",
      "Epoch: [1/40], SHFE.rb.2023-03-07.41258.h5, Loss: 1.98046509\n",
      "Epoch: [1/40], SHFE.rb.2023-03-08.41271.h5, Loss: 2.03351330\n",
      "Epoch: [1/40], SHFE.rb.2023-03-09.41316.h5, Loss: 2.05810822\n",
      "Epoch: [1/40], SHFE.rb.2023-03-10.41339.h5, Loss: 2.24143012\n",
      "Epoch: [1/40], SHFE.rb.2023-03-13.41283.h5, Loss: 1.96230319\n",
      "Epoch: [1/40], SHFE.rb.2023-03-14.41321.h5, Loss: 2.17328105\n",
      "Epoch: [1/40], SHFE.rb.2023-03-15.41321.h5, Loss: 2.19120806\n",
      "Epoch: [1/40], SHFE.rb.2023-03-16.41392.h5, Loss: 2.50917774\n",
      "Epoch: [1/40], SHFE.rb.2023-03-17.41334.h5, Loss: 2.42025734\n",
      "Epoch: [1/40], SHFE.rb.2023-03-20.41355.h5, Loss: 2.47043754\n",
      "Epoch: [1/40], SHFE.rb.2023-03-21.41336.h5, Loss: 2.14328043\n",
      "Epoch: [1/40], SHFE.rb.2023-03-22.41317.h5, Loss: 2.14560471\n",
      "Epoch: [1/40], SHFE.rb.2023-03-23.41316.h5, Loss: 2.17518653\n",
      "Epoch: [1/40], SHFE.rb.2023-03-24.41347.h5, Loss: 2.33900405\n",
      "Epoch: [1/40], SHFE.rb.2023-03-27.41167.h5, Loss: 1.98371785\n",
      "Epoch: [1/40], SHFE.rb.2023-03-28.41281.h5, Loss: 1.78897795\n",
      "Epoch: [1/40], SHFE.rb.2023-03-29.41023.h5, Loss: 1.86915358\n",
      "Epoch: [1/40], SHFE.rb.2023-03-30.41187.h5, Loss: 2.13932667\n",
      "Epoch: [1/40], SHFE.rb.2023-03-31.41193.h5, Loss: 2.10342780\n",
      "Epoch: [1/40], SHFE.rb.2023-04-03.41093.h5, Loss: 2.58614568\n",
      "Epoch: [1/40], SHFE.rb.2023-04-04.26798.h5, Loss: 2.31108950\n",
      "Epoch: [1/40], SHFE.rb.2023-04-06.41334.h5, Loss: 2.34549399\n",
      "Epoch: [1/40], SHFE.rb.2023-04-07.41146.h5, Loss: 2.08222878\n",
      "Epoch: [1/40], SHFE.rb.2023-04-10.41282.h5, Loss: 2.09970872\n",
      "Epoch: [1/40], SHFE.rb.2023-04-11.41324.h5, Loss: 2.30421458\n",
      "Epoch: [1/40], SHFE.rb.2023-04-12.41246.h5, Loss: 2.05735641\n",
      "Epoch: [1/40], SHFE.rb.2023-04-13.41328.h5, Loss: 2.21530180\n",
      "Epoch: [1/40], SHFE.rb.2023-04-14.41313.h5, Loss: 2.25993502\n",
      "Epoch: [1/40], SHFE.rb.2023-04-17.41281.h5, Loss: 2.01283876\n",
      "Epoch: [1/40], SHFE.rb.2023-04-18.41328.h5, Loss: 2.05062113\n",
      "Epoch: [1/40], SHFE.rb.2023-04-19.41358.h5, Loss: 2.31087872\n",
      "Epoch: [1/40], SHFE.rb.2023-04-20.41343.h5, Loss: 2.37393660\n",
      "Epoch: [1/40], SHFE.rb.2023-04-21.41375.h5, Loss: 2.51300224\n",
      "Epoch: [1/40], SHFE.rb.2023-04-24.41360.h5, Loss: 2.44820667\n",
      "Epoch: [1/40], SHFE.rb.2023-04-25.41340.h5, Loss: 2.35919403\n",
      "Epoch: [1/40], SHFE.rb.2023-04-26.41336.h5, Loss: 2.22724417\n",
      "Epoch: [1/40], SHFE.rb.2023-04-27.41367.h5, Loss: 2.30753953\n",
      "Epoch: [1/40], SHFE.rb.2023-04-28.26924.h5, Loss: 2.01457119\n",
      "Epoch: [1/40], SHFE.rb.2023-05-04.41390.h5, Loss: 2.27071133\n",
      "Epoch: [1/40], SHFE.rb.2023-05-05.41398.h5, Loss: 2.24590813\n",
      "Epoch: [1/40], SHFE.rb.2023-05-08.41404.h5, Loss: 2.20500506\n",
      "Epoch: [1/40], SHFE.rb.2023-05-09.41399.h5, Loss: 2.26700298\n",
      "Epoch: [1/40], SHFE.rb.2023-05-10.41384.h5, Loss: 2.62679097\n",
      "Epoch: [1/40], SHFE.rb.2023-05-11.41406.h5, Loss: 2.45748432\n",
      "Epoch: [1/40], SHFE.rb.2023-05-12.41403.h5, Loss: 2.65837289\n",
      "Epoch: [1/40], SHFE.rb.2023-05-15.41392.h5, Loss: 2.43924142\n",
      "Epoch: [1/40], SHFE.rb.2023-05-16.41392.h5, Loss: 2.33506493\n",
      "Epoch: [1/40], SHFE.rb.2023-05-17.41389.h5, Loss: 2.02725740\n",
      "Epoch: [1/40], SHFE.rb.2023-05-18.41391.h5, Loss: 2.35378779\n",
      "Epoch: [1/40], SHFE.rb.2023-05-19.41398.h5, Loss: 2.56515102\n",
      "Epoch: [1/40], SHFE.rb.2023-05-22.41403.h5, Loss: 2.43206586\n",
      "Epoch: [1/40], SHFE.rb.2023-05-23.41404.h5, Loss: 2.31169875\n",
      "Epoch: [1/40], SHFE.rb.2023-05-24.41403.h5, Loss: 2.14039553\n",
      "Epoch: [1/40], SHFE.rb.2023-05-25.41407.h5, Loss: 2.51506669\n",
      "Epoch: [1/40], SHFE.rb.2023-05-26.41402.h5, Loss: 2.45747196\n",
      "Epoch: [1/40], SHFE.rb.2023-05-29.41378.h5, Loss: 2.08624078\n",
      "Epoch: [1/40], SHFE.rb.2023-05-30.41384.h5, Loss: 2.17716593\n",
      "Epoch: [1/40], SHFE.rb.2023-05-31.41393.h5, Loss: 2.09597410\n",
      "Epoch: [1/40], SHFE.rb.2023-06-01.41353.h5, Loss: 2.27925552\n",
      "Epoch: [1/40], SHFE.rb.2023-06-02.41400.h5, Loss: 2.25160094\n",
      "Epoch: [1/40], SHFE.rb.2023-06-05.41391.h5, Loss: 2.22194311\n",
      "Epoch: [1/40], SHFE.rb.2023-06-06.41399.h5, Loss: 2.36633029\n",
      "Epoch: [2/40], SHFE.rb.2022-12-02.41243.h5, Loss: 2.15440299\n",
      "Epoch: [2/40], SHFE.rb.2022-12-05.41281.h5, Loss: 2.26996629\n",
      "Epoch: [2/40], SHFE.rb.2022-12-06.40848.h5, Loss: 2.01443804\n",
      "Epoch: [2/40], SHFE.rb.2022-12-07.41234.h5, Loss: 2.27401355\n",
      "Epoch: [2/40], SHFE.rb.2022-12-08.41207.h5, Loss: 2.10580502\n",
      "Epoch: [2/40], SHFE.rb.2022-12-09.41358.h5, Loss: 2.42809996\n",
      "Epoch: [2/40], SHFE.rb.2022-12-12.41249.h5, Loss: 2.23192758\n",
      "Epoch: [2/40], SHFE.rb.2022-12-13.41244.h5, Loss: 2.38279488\n",
      "Epoch: [2/40], SHFE.rb.2022-12-14.41187.h5, Loss: 2.19209997\n",
      "Epoch: [2/40], SHFE.rb.2022-12-15.41351.h5, Loss: 2.64219006\n",
      "Epoch: [2/40], SHFE.rb.2022-12-16.41382.h5, Loss: 2.72886080\n",
      "Epoch: [2/40], SHFE.rb.2022-12-19.41305.h5, Loss: 2.59158909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2/40], SHFE.rb.2022-12-20.41323.h5, Loss: 2.48019772\n",
      "Epoch: [2/40], SHFE.rb.2022-12-21.41294.h5, Loss: 2.46013906\n",
      "Epoch: [2/40], SHFE.rb.2022-12-22.41297.h5, Loss: 2.54279517\n",
      "Epoch: [2/40], SHFE.rb.2022-12-23.41363.h5, Loss: 2.49524128\n",
      "Epoch: [2/40], SHFE.rb.2022-12-26.41245.h5, Loss: 2.26778931\n",
      "Epoch: [2/40], SHFE.rb.2022-12-27.41203.h5, Loss: 2.29010029\n",
      "Epoch: [2/40], SHFE.rb.2022-12-28.41268.h5, Loss: 2.16378984\n",
      "Epoch: [2/40], SHFE.rb.2022-12-29.41263.h5, Loss: 2.12876244\n",
      "Epoch: [2/40], SHFE.rb.2022-12-30.26898.h5, Loss: 2.24706234\n",
      "Epoch: [2/40], SHFE.rb.2023-01-03.41159.h5, Loss: 1.88083559\n",
      "Epoch: [2/40], SHFE.rb.2023-01-04.41291.h5, Loss: 2.34520742\n",
      "Epoch: [2/40], SHFE.rb.2023-01-05.41261.h5, Loss: 2.23900049\n",
      "Epoch: [2/40], SHFE.rb.2023-01-06.41378.h5, Loss: 2.59752767\n",
      "Epoch: [2/40], SHFE.rb.2023-01-09.41028.h5, Loss: 1.98125561\n",
      "Epoch: [2/40], SHFE.rb.2023-01-10.41074.h5, Loss: 2.01208957\n",
      "Epoch: [2/40], SHFE.rb.2023-01-11.41239.h5, Loss: 1.87653834\n",
      "Epoch: [2/40], SHFE.rb.2023-01-12.41161.h5, Loss: 2.00504718\n",
      "Epoch: [2/40], SHFE.rb.2023-01-13.41112.h5, Loss: 1.63617324\n",
      "Epoch: [2/40], SHFE.rb.2023-01-16.41176.h5, Loss: 1.97532004\n",
      "Epoch: [2/40], SHFE.rb.2023-01-17.41091.h5, Loss: 1.82699675\n",
      "Epoch: [2/40], SHFE.rb.2023-01-18.40868.h5, Loss: 1.85472060\n",
      "Epoch: [2/40], SHFE.rb.2023-01-19.40989.h5, Loss: 1.94587929\n",
      "Epoch: [2/40], SHFE.rb.2023-01-20.26721.h5, Loss: 1.96075550\n",
      "Epoch: [2/40], SHFE.rb.2023-01-30.41190.h5, Loss: 2.17793214\n",
      "Epoch: [2/40], SHFE.rb.2023-01-31.41277.h5, Loss: 2.26127579\n",
      "Epoch: [2/40], SHFE.rb.2023-02-01.41291.h5, Loss: 2.37610890\n",
      "Epoch: [2/40], SHFE.rb.2023-02-02.41255.h5, Loss: 2.31470774\n",
      "Epoch: [2/40], SHFE.rb.2023-02-03.41366.h5, Loss: 2.41575048\n",
      "Epoch: [2/40], SHFE.rb.2023-02-06.41310.h5, Loss: 2.41413142\n",
      "Epoch: [2/40], SHFE.rb.2023-02-07.41324.h5, Loss: 2.45959673\n",
      "Epoch: [2/40], SHFE.rb.2023-02-08.41316.h5, Loss: 2.19176213\n",
      "Epoch: [2/40], SHFE.rb.2023-02-09.41266.h5, Loss: 2.01470797\n",
      "Epoch: [2/40], SHFE.rb.2023-02-10.41273.h5, Loss: 2.41703762\n",
      "Epoch: [2/40], SHFE.rb.2023-02-13.41342.h5, Loss: 2.19976633\n",
      "Epoch: [2/40], SHFE.rb.2023-02-14.41311.h5, Loss: 2.10734744\n",
      "Epoch: [2/40], SHFE.rb.2023-02-15.41189.h5, Loss: 1.96604465\n",
      "Epoch: [2/40], SHFE.rb.2023-02-16.41329.h5, Loss: 2.16570577\n",
      "Epoch: [2/40], SHFE.rb.2023-02-17.41290.h5, Loss: 2.02496939\n",
      "Epoch: [2/40], SHFE.rb.2023-02-20.41254.h5, Loss: 1.85728035\n",
      "Epoch: [2/40], SHFE.rb.2023-02-21.41275.h5, Loss: 2.17755856\n",
      "Epoch: [2/40], SHFE.rb.2023-02-22.41338.h5, Loss: 2.00718006\n",
      "Epoch: [2/40], SHFE.rb.2023-02-23.41315.h5, Loss: 2.29142132\n",
      "Epoch: [2/40], SHFE.rb.2023-02-24.41341.h5, Loss: 2.51285412\n",
      "Epoch: [2/40], SHFE.rb.2023-02-27.41294.h5, Loss: 2.06712922\n",
      "Epoch: [2/40], SHFE.rb.2023-02-28.41244.h5, Loss: 2.03666606\n",
      "Epoch: [2/40], SHFE.rb.2023-03-01.41250.h5, Loss: 2.01436004\n",
      "Epoch: [2/40], SHFE.rb.2023-03-02.41145.h5, Loss: 1.70732465\n",
      "Epoch: [2/40], SHFE.rb.2023-03-03.41219.h5, Loss: 1.87928267\n",
      "Epoch: [2/40], SHFE.rb.2023-03-06.41227.h5, Loss: 1.96242873\n",
      "Epoch: [2/40], SHFE.rb.2023-03-07.41258.h5, Loss: 1.98086601\n",
      "Epoch: [2/40], SHFE.rb.2023-03-08.41271.h5, Loss: 2.03255180\n",
      "Epoch: [2/40], SHFE.rb.2023-03-09.41316.h5, Loss: 2.05942425\n",
      "Epoch: [2/40], SHFE.rb.2023-03-10.41339.h5, Loss: 2.24027062\n",
      "Epoch: [2/40], SHFE.rb.2023-03-13.41283.h5, Loss: 1.96440847\n",
      "Epoch: [2/40], SHFE.rb.2023-03-14.41321.h5, Loss: 2.17335109\n",
      "Epoch: [2/40], SHFE.rb.2023-03-15.41321.h5, Loss: 2.18911499\n",
      "Epoch: [2/40], SHFE.rb.2023-03-16.41392.h5, Loss: 2.50726661\n",
      "Epoch: [2/40], SHFE.rb.2023-03-17.41334.h5, Loss: 2.41962073\n",
      "Epoch: [2/40], SHFE.rb.2023-03-20.41355.h5, Loss: 2.46824367\n",
      "Epoch: [2/40], SHFE.rb.2023-03-21.41336.h5, Loss: 2.14330895\n",
      "Epoch: [2/40], SHFE.rb.2023-03-22.41317.h5, Loss: 2.14579675\n",
      "Epoch: [2/40], SHFE.rb.2023-03-23.41316.h5, Loss: 2.17371313\n",
      "Epoch: [2/40], SHFE.rb.2023-03-24.41347.h5, Loss: 2.34014215\n",
      "Epoch: [2/40], SHFE.rb.2023-03-27.41167.h5, Loss: 1.98356732\n",
      "Epoch: [2/40], SHFE.rb.2023-03-28.41281.h5, Loss: 1.79034775\n",
      "Epoch: [2/40], SHFE.rb.2023-03-29.41023.h5, Loss: 1.86769026\n",
      "Epoch: [2/40], SHFE.rb.2023-03-30.41187.h5, Loss: 2.13982817\n",
      "Epoch: [2/40], SHFE.rb.2023-03-31.41193.h5, Loss: 2.10329695\n",
      "Epoch: [2/40], SHFE.rb.2023-04-03.41093.h5, Loss: 2.58434960\n",
      "Epoch: [2/40], SHFE.rb.2023-04-04.26798.h5, Loss: 2.30941236\n",
      "Epoch: [2/40], SHFE.rb.2023-04-06.41334.h5, Loss: 2.34435529\n",
      "Epoch: [2/40], SHFE.rb.2023-04-07.41146.h5, Loss: 2.07996420\n",
      "Epoch: [2/40], SHFE.rb.2023-04-10.41282.h5, Loss: 2.09751925\n",
      "Epoch: [2/40], SHFE.rb.2023-04-11.41324.h5, Loss: 2.30405662\n",
      "Epoch: [2/40], SHFE.rb.2023-04-12.41246.h5, Loss: 2.05630793\n",
      "Epoch: [2/40], SHFE.rb.2023-04-13.41328.h5, Loss: 2.21568813\n",
      "Epoch: [2/40], SHFE.rb.2023-04-14.41313.h5, Loss: 2.25934311\n",
      "Epoch: [2/40], SHFE.rb.2023-04-17.41281.h5, Loss: 2.01344613\n",
      "Epoch: [2/40], SHFE.rb.2023-04-18.41328.h5, Loss: 2.05187263\n",
      "Epoch: [2/40], SHFE.rb.2023-04-19.41358.h5, Loss: 2.31028323\n",
      "Epoch: [2/40], SHFE.rb.2023-04-20.41343.h5, Loss: 2.37339898\n",
      "Epoch: [2/40], SHFE.rb.2023-04-21.41375.h5, Loss: 2.50930551\n",
      "Epoch: [2/40], SHFE.rb.2023-04-24.41360.h5, Loss: 2.44752862\n",
      "Epoch: [2/40], SHFE.rb.2023-04-25.41340.h5, Loss: 2.35848363\n",
      "Epoch: [2/40], SHFE.rb.2023-04-26.41336.h5, Loss: 2.22649514\n",
      "Epoch: [2/40], SHFE.rb.2023-04-27.41367.h5, Loss: 2.30474172\n",
      "Epoch: [2/40], SHFE.rb.2023-04-28.26924.h5, Loss: 2.01398287\n",
      "Epoch: [2/40], SHFE.rb.2023-05-04.41390.h5, Loss: 2.26826529\n",
      "Epoch: [2/40], SHFE.rb.2023-05-05.41398.h5, Loss: 2.24665996\n",
      "Epoch: [2/40], SHFE.rb.2023-05-08.41404.h5, Loss: 2.20715848\n",
      "Epoch: [2/40], SHFE.rb.2023-05-09.41399.h5, Loss: 2.26681684\n",
      "Epoch: [2/40], SHFE.rb.2023-05-10.41384.h5, Loss: 2.62249673\n",
      "Epoch: [2/40], SHFE.rb.2023-05-11.41406.h5, Loss: 2.45683131\n",
      "Epoch: [2/40], SHFE.rb.2023-05-12.41403.h5, Loss: 2.65765404\n",
      "Epoch: [2/40], SHFE.rb.2023-05-15.41392.h5, Loss: 2.44020128\n",
      "Epoch: [2/40], SHFE.rb.2023-05-16.41392.h5, Loss: 2.33427053\n",
      "Epoch: [2/40], SHFE.rb.2023-05-17.41389.h5, Loss: 2.02786448\n",
      "Epoch: [2/40], SHFE.rb.2023-05-18.41391.h5, Loss: 2.35337357\n",
      "Epoch: [2/40], SHFE.rb.2023-05-19.41398.h5, Loss: 2.56447924\n",
      "Epoch: [2/40], SHFE.rb.2023-05-22.41403.h5, Loss: 2.43108012\n"
     ]
    }
   ],
   "source": [
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "\n",
    "step = 0\n",
    "# training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "#     total_size = 0\n",
    "    \n",
    "#     for ix in range(0, len(dfs_x)-20):\n",
    "    for f in os.listdir(SAVE_PATH):\n",
    "        if not f.startswith('SHFE.rb'):\n",
    "            continue\n",
    "        \n",
    "        store = pd.HDFStore(os.path.join(SAVE_PATH, f))\n",
    "        X = normalize(store['X'].values)\n",
    "#         y = store['y'].values * 100\n",
    "        y = store['y'].values[:,1]\n",
    "        store.close()\n",
    "        \n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=False)\n",
    "        X_train = torch.tensor(X, dtype=torch.float32, device=0)\n",
    "        y_train = torch.tensor(y, dtype=torch.float32, device=0).reshape(-1, 1)\n",
    "        \n",
    "        total_size = X_train.shape[0]\n",
    "        batch_start = torch.arange(0, total_size, batch_size)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        for i, start in enumerate(batch_start, 1):\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "#             y_pred, fc1, conv1, conv2, conv3, out_fc1 = model(X_batch) # .reshape(-1, 1, 300)\n",
    "            y_pred, fc1, conv1, conv2, conv3, dropout, out_fc1 = model(X_batch)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "\n",
    "            running_loss += loss.item() * X_batch.shape[0]\n",
    "\n",
    "#             step = epoch * len(batch_start) + i\n",
    "#             writer.add_scalar('loss', loss.item(), step)\n",
    "\n",
    "#             if i % 5000 == 0:\n",
    "\n",
    "        step += total_size\n",
    "        \n",
    "        writer.add_scalar('loss/loss', loss.item(), step)\n",
    "        writer.add_scalar('loss/total_loss', running_loss / (total_size), step)\n",
    "        \n",
    "        writer.add_histogram('deep/01_x', torchvision.utils.make_grid(X_batch).data.cpu(), step)\n",
    "        writer.add_histogram('deep/02_fc1', torchvision.utils.make_grid(fc1).data.cpu(), step)\n",
    "        writer.add_histogram('deep/03_conv1', torchvision.utils.make_grid(conv1).data.cpu(), step)\n",
    "        writer.add_histogram('deep/04_conv2', torchvision.utils.make_grid(conv2).data.cpu(), step)\n",
    "        writer.add_histogram('deep/05_conv3', torchvision.utils.make_grid(conv3).data.cpu(), step)\n",
    "        writer.add_histogram('deep/06_dropout', torchvision.utils.make_grid(dropout).data.cpu(), step)\n",
    "        writer.add_histogram('deep/07_out_fc1', torchvision.utils.make_grid(out_fc1).data.cpu(), step)\n",
    "\n",
    "        writer.add_histogram('deep/09_pred', torchvision.utils.make_grid(y_pred).data.cpu(), step)\n",
    "\n",
    "#         print('Epoch: [{}/{}], [{}/{}], Loss: {:.8f}'.format(epoch + 1, n_epochs, start+10, total_size, running_loss / (X_batch.shape[0] * i)))\n",
    "#         print('Epoch: [{}/{}], {}, Loss: {:.8f}'.format(epoch + 1, n_epochs, ix, running_loss / (total_size)))\n",
    "        print('Epoch: [{}/{}], {}, Loss: {:.8f}'.format(epoch + 1, n_epochs, f, running_loss / (total_size)))\n",
    "    \n",
    "#     for ix in range(len(dfs_x)-20, len(dfs_x)):\n",
    "#         X_test = torch.tensor(dfs_x[ix].values, dtype=torch.float32, device=0)\n",
    "#         y_test = torch.tensor(dfs_y[ix].values, dtype=torch.float32, device=0).reshape(-1, 1)\n",
    "        \n",
    "#         batch_test_start = torch.arange(0, len(X_test), batch_size)\n",
    "        \n",
    "#         # evaluate accuracy at end of each epoch\n",
    "#         model.eval()\n",
    "#         eval_loss = 0.0\n",
    "#         for i, start in enumerate(batch_test_start, 1):\n",
    "#             X_batch_test = X_test[start:start+batch_size]\n",
    "#             y_batch_test = y_test[start:start+batch_size]\n",
    "\n",
    "#             y_pred, fc1, conv1, conv2, conv3, out_fc1 = model(X_batch_test)\n",
    "#             loss = loss_fn(y_pred, y_batch_test).cpu().detach().numpy()\n",
    "\n",
    "#             eval_loss += loss.item() * X_batch_test.shape[0]\n",
    "\n",
    "#         eval_loss = eval_loss / X_test.shape[0]\n",
    "#         print('eval loss {:.8f}'.format(eval_loss))\n",
    "#         history.append(eval_loss)\n",
    "#         if eval_loss < best_mse:\n",
    "#             best_mse = eval_loss\n",
    "#             best_weights = copy.deepcopy(model.state_dict())\n",
    " \n",
    "# # restore model and return best accuracy\n",
    "# model.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cd0476",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15876\\3562620839.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24833c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17296\\4075987148.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "686bdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'D:/option/models/model_0608_01.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4b143e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.14\n",
      "RMSE: 0.38\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ec39341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3e1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40c0db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca538ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012f691b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035c879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc20b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f1c808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efc730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bce00f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9c885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-banks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-reservation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
